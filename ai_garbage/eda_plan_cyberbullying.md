# EDA Plan: Cyberbullying Classification Dataset

## üìã –ö–æ–Ω—Ç–µ–∫—Å—Ç

**–î–∞—Ç–∞—Å–µ—Ç:** Cyberbullying Classification  
**URL:** https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification  
**–û–±—ä—ë–º:** 47,000+ —Ç–≤–∏—Ç–æ–≤  
**–ö–æ–ª–æ–Ω–∫–∏:** `tweet_text`, `cyberbullying_type`  
**–ó–∞–¥–∞—á–∞:** –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ (Toxic / Non-Toxic)

---

## 1. –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ —Å–ø–æ—Å–æ–±–æ–≤ –ø–æ–ª—É—á–µ–Ω–∏—è

### 1.1. –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|----------|
| **–ù–∞–∑–≤–∞–Ω–∏–µ** | Cyberbullying Classification |
| **–ê–≤—Ç–æ—Ä** | andrewmvd |
| **–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞** | Kaggle |
| **URL** | https://www.kaggle.com/datasets/andrewmvd/cyberbullying-classification |
| **–õ–∏—Ü–µ–Ω–∑–∏—è** | Kaggle (–ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–µ—Ä–µ–¥ commercial use) |

### 1.2. –°–ø–æ—Å–æ–± –ø–æ–ª—É—á–µ–Ω–∏—è

```bash
# –í–∞—Ä–∏–∞–Ω—Ç 1: –°–∫–∞—á–∞—Ç—å —á–µ—Ä–µ–∑ Kaggle CLI
kaggle datasets download -d andrewmvd/cyberbullying-classification
unzip cyberbullying-classification.zip

# –í–∞—Ä–∏–∞–Ω—Ç 2: –°–∫–∞—á–∞—Ç—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
# –í–∞—Ä–∏–∞–Ω—Ç 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kaggle API –≤ Python
```

```python
# –ó–∞–≥—Ä—É–∑–∫–∞ –≤ Python
import pandas as pd

df = pd.read_csv('cyberbullying.csv')
# –∏–ª–∏
df = pd.read_csv('cyberbullying_classification.csv')  # —Ç–æ—á–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
```

### 1.3. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (–æ–∂–∏–¥–∞–µ–º–∞—è)

```python
# –û–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏
df.columns
# ['tweet_text', 'cyberbullying_type']

# –û–∂–∏–¥–∞–µ–º—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
df.dtypes
# tweet_text          object
# cyberbullying_type  object
```

---

## 2. EDA (Exploratory Data Analysis)

### 2.1. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

```python
import pandas as pd
import numpy as np

# –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
df.shape  # (–æ–∂–∏–¥–∞–µ–º–æ: ~47000, 2)
df.info()
df.head(10)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
print(f"–¢–∏–ø tweet_text: {df['tweet_text'].dtype}")
print(f"–¢–∏–ø cyberbullying_type: {df['cyberbullying_type'].dtype}")
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –∏ –∫–æ–ª–æ–Ω–æ–∫
- [ ] –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö (object –¥–ª—è —Ç–µ–∫—Å—Ç–∞ –∏ –∫–ª–∞—Å—Å–∞)
- [ ] –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞

---

### 2.2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (Class Distribution)

```python
import matplotlib.pyplot as plt
import seaborn as sns

# –ò—Å—Ö–æ–¥–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (6 –∫–ª–∞—Å—Å–æ–≤)
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –∫–ª–∞—Å—Å–∞–º:")
print(df['cyberbullying_type'].value_counts())
print(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã: {df['cyberbullying_type'].unique()}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='cyberbullying_type', order=df['cyberbullying_type'].value_counts().index)
plt.xticks(rotation=45)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º –∫–∏–±–µ—Ä–±—É–ª–ª–∏–Ω–≥–∞ (–∏—Å—Ö–æ–¥–Ω–æ–µ)')
plt.xlabel('–¢–∏–ø –±—É–ª–ª–∏–Ω–≥–∞')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
plt.tight_layout()
plt.savefig('eda/class_distribution_original.png')
plt.show()

# –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ is_toxic
df['is_toxic'] = df['cyberbullying_type'].apply(
    lambda x: 0 if x == 'not_cyberbullying' else 1
)

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞
print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞ (binary):")
print(df['is_toxic'].value_counts())
print(f"\n–ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {df['is_toxic'].value_counts().ratio:.2f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–Ω–∞—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
plt.figure(figsize=(8, 5))
sns.countplot(data=df, x='is_toxic')
plt.xticks([0, 1], ['Not Toxic', 'Toxic'])
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º (–±–∏–Ω–∞—Ä–Ω–æ–µ)')
plt.xlabel('–ö–ª–∞—Å—Å')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
plt.tight_layout()
plt.savefig('eda/class_distribution_binary.png')
plt.show()
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (–æ–∂–∏–¥–∞–µ–º–æ: 6)
- [ ] –ë–∞–ª–∞–Ω—Å –∏—Å—Ö–æ–¥–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ (~8000 –∫–∞–∂–¥—ã–π)
- [ ] –ë–∞–ª–∞–Ω—Å –ø–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞ (–æ–∂–∏–¥–∞–µ–º–æ: ~8K not_toxic, ~39K toxic = —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ 5:1)
- [ ] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

**–ú–µ—Ç—Ä–∏–∫–∏ –±–∞–ª–∞–Ω—Å–∞:**
```python
toxic_count = df[df['is_toxic'] == 1].shape[0]
non_toxic_count = df[df['is_toxic'] == 0].shape[0]
imbalance_ratio = toxic_count / non_toxic_count
print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {imbalance_ratio:.2f}:1 (toxic:not_toxic)")
```

---

### 2.3. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤

```python
# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω
df['text_length'] = df['tweet_text'].apply(len)
df['word_count'] = df['tweet_text'].apply(lambda x: len(x.split()))

print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤:")
print(df[['text_length', 'word_count']].describe())

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω (–æ–±—â–µ–µ)
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(df['text_length'], bins=50, edgecolor='black', alpha=0.7)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤ (—Å–∏–º–≤–æ–ª—ã)')
plt.xlabel('–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã)')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
plt.axvline(df['text_length'].mean(), color='r', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df["text_length"].mean():.0f}')
plt.legend()

plt.subplot(1, 2, 2)
plt.hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤ (—Å–ª–æ–≤–∞)')
plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
plt.axvline(df['word_count'].mean(), color='r', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {df["word_count"].mean():.0f}')
plt.legend()

plt.tight_layout()
plt.savefig('eda/text_length_distribution.png')
plt.show()

# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω –ø–æ –∫–ª–∞—Å—Å–∞–º
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.boxplot(data=df, x='is_toxic', y='text_length')
plt.xticks([0, 1], ['Not Toxic', 'Toxic'])
plt.title('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º (—Å–∏–º–≤–æ–ª—ã)')
plt.xlabel('–ö–ª–∞—Å—Å')
plt.ylabel('–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã)')

plt.subplot(1, 2, 2)
sns.boxplot(data=df, x='is_toxic', y='word_count')
plt.xticks([0, 1], ['Not Toxic', 'Toxic'])
plt.title('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º (—Å–ª–æ–≤–∞)')
plt.xlabel('–ö–ª–∞—Å—Å')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤')

plt.tight_layout()
plt.savefig('eda/text_length_by_class.png')
plt.show()

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['text_length'].describe())
print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['word_count'].describe())
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ú–∏–Ω/–º–∞–∫—Å/—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
- [ ] –í—ã–±—Ä–æ—Å—ã (–æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ/–∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã)
- [ ] –†–∞–∑–ª–∏—á–∏—è –¥–ª–∏–Ω –º–µ–∂–¥—É toxic –∏ not_toxic
- [ ] –í—ã–±–æ—Ä –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 280 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è Twitter)

---

### 2.4. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (Missing Values)

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
missing_stats = df.isnull().sum()
missing_pct = (df.isnull().sum() / len(df)) * 100

missing_df = pd.DataFrame({
    '–ü—Ä–æ–ø—É—Å–∫–∏': missing_stats,
    '–ü—Ä–æ—Ü–µ–Ω—Ç': missing_pct
})
print("–ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:")
print(missing_df)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(8, 5))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis', yticklabels=False)
plt.title('–ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö')
plt.savefig('eda/missing_values_heatmap.png')
plt.show()
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ `tweet_text` (–æ–∂–∏–¥–∞–µ–º–æ: 0)
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ `cyberbullying_type` (–æ–∂–∏–¥–∞–µ–º–æ: 0)
- [ ] –î–æ–ø—É—Å—Ç–∏–º–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤: 0% (–æ–±–∞ –ø–æ–ª—è required)

**–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –ø—Ä–æ–ø—É—Å–∫–∞—Ö:**
```python
# –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
df = df.dropna(subset=['tweet_text'])  # –£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
# –∏–ª–∏
df['tweet_text'] = df['tweet_text'].fillna('')  # –ó–∞–ø–æ–ª–Ω–∏—Ç—å –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π

# –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–ª–∞—Å—Å–µ
df = df.dropna(subset=['cyberbullying_type'])  # –£–¥–∞–ª–∏—Ç—å (–∫–ª–∞—Å—Å –∫—Ä–∏—Ç–∏—á–µ–Ω)
```

---

### 2.5. –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤

```python
# –î—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤
duplicate_texts = df['tweet_text'].duplicated().sum()
print(f"–î—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤: {duplicate_texts} ({duplicate_texts/len(df)*100:.2f}%)")

# –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ–ª–Ω—ã—Ö —Å—Ç—Ä–æ–∫
duplicate_rows = df.duplicated().sum()
print(f"–ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫: {duplicate_rows} ({duplicate_rows/len(df)*100:.2f}%)")

# –¢–æ–ø –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤
if duplicate_texts > 0:
    duplicate_df = df[df['tweet_text'].duplicated(keep=False)]
    print("\n–¢–æ–ø-10 –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ–∫—Å—Ç–æ–≤:")
    print(duplicate_df['tweet_text'].value_counts().head(10))
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤
- [ ] –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å—Ç—Ä–æ–∫
- [ ] –î–æ–ø—É—Å—Ç–∏–º–∞—è –¥–æ–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: < 5%

**–î–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö:**
```python
# –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
df = df.drop_duplicates(subset=['tweet_text'])
# –∏–ª–∏
df = df.drop_duplicates()  # –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
```

---

### 2.6. –¢–æ–ø —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º (Word Frequency Analysis)

```python
from collections import Counter
import re

def preprocess_text(text):
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–≤"""
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)  # –£–¥–∞–ª–∏—Ç—å URL
    text = re.sub(r'@\w+|#\w+', '', text)  # –£–¥–∞–ª–∏—Ç—å @mentions –∏ #hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # –û—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã
    words = text.split()
    words = [w for w in words if len(w) > 2]  # –£–¥–∞–ª–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    return words

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
toxic_texts = df[df['is_toxic'] == 1]['tweet_text']
non_toxic_texts = df[df['is_toxic'] == 0]['tweet_text']

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
toxic_words = []
for text in toxic_texts:
    toxic_words.extend(preprocess_text(text))

non_toxic_words = []
for text in non_toxic_texts:
    non_toxic_words.extend(preprocess_text(text))

# –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
toxic_word_counts = Counter(toxic_words)
non_toxic_word_counts = Counter(non_toxic_words)

# –¢–æ–ø-20 —Å–ª–æ–≤
print("–¢–æ–ø-20 —Å–ª–æ–≤ –≤ TOXIC –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö:")
for word, count in toxic_word_counts.most_common(20):
    print(f"  {word}: {count}")

print("\n–¢–æ–ø-20 —Å–ª–æ–≤ –≤ NOT_TOXIC –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö:")
for word, count in non_toxic_word_counts.most_common(20):
    print(f"  {word}: {count}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (Word Cloud)
from wordcloud import WordCloud

plt.figure(figsize=(15, 6))

plt.subplot(1, 2, 1)
wc_toxic = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(toxic_words))
plt.imshow(wc_toxic, interpolation='bilinear')
plt.title('Toxic Comments')
plt.axis('off')

plt.subplot(1, 2, 2)
wc_non_toxic = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(non_toxic_words))
plt.imshow(wc_non_toxic, interpolation='bilinear')
plt.title('Not Toxic Comments')
plt.axis('off')

plt.tight_layout()
plt.savefig('eda/wordcloud_comparison.png')
plt.show()
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –¢–æ–ø —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
- [ ] –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ (–æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è, —É–≥—Ä–æ–∑—ã)
- [ ] –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
- [ ] –ù–∞–ª–∏—á–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ (I, you, the, etc.)

---

### 2.7. –ê–Ω–∞–ª–∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
df['has_url'] = df['tweet_text'].str.contains(r'http\S+|www\S+', regex=True)
df['has_mention'] = df['tweet_text'].str.contains(r'@\w+', regex=True)
df['has_hashtag'] = df['tweet_text'].str.contains(r'#\w+', regex=True)
df['has_caps'] = df['tweet_text'].apply(lambda x: x.isupper())
df['caps_ratio'] = df['tweet_text'].apply(lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
print("–ù–∞–ª–∏—á–∏–µ URL –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['has_url'].mean())

print("\n–ù–∞–ª–∏—á–∏–µ @mention –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['has_mention'].mean())

print("\n–ù–∞–ª–∏—á–∏–µ #hashtag –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['has_hashtag'].mean())

print("\n–î–æ–ª—è CAPS –ø–æ –∫–ª–∞—Å—Å–∞–º:")
print(df.groupby('is_toxic')['caps_ratio'].mean())
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ù–∞–ª–∏—á–∏–µ URL (–≤–æ–∑–º–æ–∂–Ω—ã–π —Å–ø–∞–º)
- [ ] –ù–∞–ª–∏—á–∏–µ @mentions (–æ–±—Ä–∞—â–µ–Ω–∏—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º)
- [ ] –ù–∞–ª–∏—á–∏–µ #hashtags (—Ç–µ–º–∞—Ç–∏–∫–∏)
- [ ] CAPS LOCK (–∞–≥—Ä–µ—Å—Å–∏—è)

---

### 2.8. –ü—Ä–∏–º–µ—Ä—ã –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (Manual Review)

```python
# –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
print("=" * 80)
print("–ü–†–ò–ú–ï–†–´ TOXIC –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (5 —Å–ª—É—á–∞–π–Ω—ã—Ö):")
print("=" * 80)
toxic_sample = df[df['is_toxic'] == 1]['tweet_text'].sample(5, random_state=42)
for i, text in enumerate(toxic_sample, 1):
    print(f"\n{i}. {text}")

# –ü—Ä–∏–º–µ—Ä—ã –Ω–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
print("\n" + "=" * 80)
print("–ü–†–ò–ú–ï–†–´ NOT_TOXIC –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (5 —Å–ª—É—á–∞–π–Ω—ã—Ö):")
print("=" * 80)
non_toxic_sample = df[df['is_toxic'] == 0]['tweet_text'].sample(5, random_state=42)
for i, text in enumerate(non_toxic_sample, 1):
    print(f"\n{i}. {text}")

# –ü—Ä–∏–º–µ—Ä—ã –ø–æ–≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–ª–æ–Ω–∫—É —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é, –µ—Å–ª–∏ –µ—Å—Ç—å)
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ç–∫–∏ (—Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ 5-10 –ø—Ä–∏–º–µ—Ä–æ–≤)
- [ ] –ü–æ–≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ (—Å–ª–æ–∂–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å)
- [ ] –ù–∞–ª–∏—á–∏–µ —à—É–º–∞ –≤ —Ä–∞–∑–º–µ—Ç–∫–µ

---

### 2.9. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö (Data Leakage)

```python
# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—É–∂–µ —Å–¥–µ–ª–∞–Ω–æ –≤—ã—à–µ)
print(f"–î—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–æ–≤: {duplicate_texts}")

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ª–æ–∂–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
# –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Å–µ
toxic_vocab = set(toxic_words)
non_toxic_vocab = set(non_toxic_words)

unique_toxic = toxic_vocab - non_toxic_vocab
unique_non_toxic = non_toxic_vocab - toxic_vocab

print(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ —Ç–æ–ª—å–∫–æ –≤ toxic: {len(unique_toxic)}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ —Ç–æ–ª—å–∫–æ –≤ non_toxic: {len(unique_non_toxic)}")

# –¢–æ–ø —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
print("\n–¢–æ–ø-10 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è toxic:")
for word in list(unique_toxic)[:10]:
    count = toxic_word_counts[word]
    print(f"  {word}: {count}")

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω—É—é —É—Ç–µ—á–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å timestamp)
# –í —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ timestamp –Ω–µ—Ç, –Ω–æ –µ—Å–ª–∏ –±—ã –±—ã–ª:
# df.sort_values('timestamp') –∏ —Å–ø–ª–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –î—É–±–ª–∏–∫–∞—Ç—ã –º–µ–∂–¥—É train/test (—É–¥–∞–ª–∏—Ç—å –ø–µ—Ä–µ–¥ —Å–ø–ª–∏—Ç–æ–º)
- [ ] –õ–æ–∂–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ (—Å–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å –∑–∞–ø–æ–º–Ω–∏—Ç)
- [ ] –í—Ä–µ–º–µ–Ω–Ω–∞—è —É—Ç–µ—á–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å timestamp)

---

### 2.10. –Ø–∑—ã–∫–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (Language Detection)

```python
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —è–∑—ã–∫–∞ (–≤—ã–±–æ—Ä–æ—á–Ω–æ, —Ç.–∫. langdetect –º–µ–¥–ª–µ–Ω–Ω—ã–π)
from langdetect import detect

# –°–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
sample_df = df.sample(1000, random_state=42)
sample_df['language'] = sample_df['tweet_text'].apply(lambda x: detect(x) if len(x) > 10 else 'unknown')

print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤ (–≤—ã–±–æ—Ä–∫–∞ 1000):")
print(sample_df['language'].value_counts())

# –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ-English –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
non_english = sample_df[sample_df['language'] != 'en']
print(f"\n–ù–µ-English –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏: {len(non_english)} ({len(non_english)/len(sample_df)*100:.1f}%)")
```

**–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:**
- [ ] –û—Å–Ω–æ–≤–Ω–æ–π —è–∑—ã–∫ (–æ–∂–∏–¥–∞–µ–º–æ: English)
- [ ] –î–æ–ª—è –Ω–µ-English –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
- [ ] –†–µ—à–µ–Ω–∏–µ: —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ

---

### 2.11. –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ EDA

```python
# –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
print("=" * 80)
print("–°–í–û–î–ù–´–ô –û–¢–ß–Å–¢ EDA")
print("=" * 80)

print(f"\n1. –û–±—ä—ë–º –¥–∞–Ω–Ω—ã—Ö: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
print(f"2. –ö–ª–∞—Å—Å—ã: {df['cyberbullying_type'].nunique()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
print(f"3. –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (binary): {df['is_toxic'].value_counts().to_dict()}")
print(f"4. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {imbalance_ratio:.2f}:1")
print(f"5. –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {df['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"6. –ü—Ä–æ–ø—É—Å–∫–∏: {df.isnull().sum().sum()}")
print(f"7. –î—É–±–ª–∏–∫–∞—Ç—ã: {duplicate_texts} ({duplicate_texts/len(df)*100:.2f}%)")
print(f"8. –î–æ–ª—è URL: {df['has_url'].mean()*100:.1f}%")
print(f"9. –î–æ–ª—è CAPS (>50%): {(df['caps_ratio'] > 0.5).mean()*100:.1f}%")
```

---

## 3. Data Contract

### 3.1. –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö

| feature_name | dtype | required | –æ–ø–∏—Å–∞–Ω–∏–µ | –¥–∏–∞–ø–∞–∑–æ–Ω/–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è |
|:------------ |:------ | -------- | -------- | ------------------- |
| tweet_text | object | True | –¢–µ–∫—Å—Ç —Ç–≤–∏—Ç–∞ | 1-500 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–µ –ø—É—Å—Ç–æ–π |
| cyberbullying_type | object | True | –¢–∏–ø –±—É–ª–ª–∏–Ω–≥–∞ | {not_cyberbullying, age, ethnicity, gender, religion, sexual_orientation, other_cyberbullying} |
| is_toxic | int64 | True | –ë–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ (–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–π) | {0, 1} |

### 3.2. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ |
|----------|----------|-------------|
| **–î–æ–ø—É—Å—Ç–∏–º–∞—è –¥–æ–ª—è –ø—Ä–æ–ø—É—Å–∫–æ–≤** | 0% | –û–±–∞ –ø–æ–ª—è required |
| **–î–æ–ø—É—Å—Ç–∏–º–∞—è –¥–æ–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤** | < 5% | –ò–Ω–∞—á–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ |
| **–ú–∏–Ω. –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞** | 1 —Ç–æ–∫–µ–Ω | –ü—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã |
| **–ú–∞–∫—Å. –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞** | 512 —Ç–æ–∫–µ–Ω–æ–≤ | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π |
| **–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–≤–µ–∂–µ—Å—Ç–∏** | –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ | –Ø–∑—ã–∫ —Å—Ç–∞–±–∏–ª–µ–Ω |
| **–ü–µ—Ä–∏–æ–¥–∏—á–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è** | –ï–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ | –î–ª—è –æ–±—É—á–µ–Ω–∏—è v1 |
| **–Ø–∑—ã–∫** | English | –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–µ–∫—Ü–∏—è non-English |

### 3.3. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (Data Validation)

```python
def validate_data(df):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è Data Contract"""
    errors = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–æ–Ω–æ–∫
    required_cols = ['tweet_text', 'cyberbullying_type']
    for col in required_cols:
        if col not in df.columns:
            errors.append(f"Missing column: {col}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    if df['tweet_text'].isnull().sum() > 0:
        errors.append(f"Null values in tweet_text: {df['tweet_text'].isnull().sum()}")
    
    if df['cyberbullying_type'].isnull().sum() > 0:
        errors.append(f"Null values in cyberbullying_type: {df['cyberbullying_type'].isnull().sum()}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    valid_types = {'not_cyberbullying', 'age', 'ethnicity', 'gender', 'religion', 'sexual_orientation', 'other_cyberbullying'}
    actual_types = set(df['cyberbullying_type'].unique())
    invalid_types = actual_types - valid_types
    if invalid_types:
        errors.append(f"Invalid cyberbullying_type values: {invalid_types}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
    empty_texts = (df['tweet_text'].str.len() == 0).sum()
    if empty_texts > 0:
        errors.append(f"Empty tweet_text: {empty_texts}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    duplicate_pct = df['tweet_text'].duplicated().sum() / len(df) * 100
    if duplicate_pct > 5:
        errors.append(f"High duplicate rate: {duplicate_pct:.2f}%")
    
    if errors:
        print("DATA VALIDATION FAILED:")
        for error in errors:
            print(f"  ‚ùå {error}")
        return False
    else:
        print("DATA VALIDATION PASSED ‚úÖ")
        return True
```

---

## 4. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ—Ç—á—ë—Ç–∞

### –°–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤:

1. **class_distribution_original.png** ‚Äî –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 6 –∏—Å—Ö–æ–¥–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
2. **class_distribution_binary.png** ‚Äî –ë–∏–Ω–∞—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (toxic/not_toxic)
3. **text_length_distribution.png** ‚Äî –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –¥–ª–∏–Ω —Ç–µ–∫—Å—Ç–æ–≤ (2 –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞)
4. **text_length_by_class.png** ‚Äî Box plot –¥–ª–∏–Ω –ø–æ –∫–ª–∞—Å—Å–∞–º
5. **missing_values_heatmap.png** ‚Äî –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
6. **wordcloud_comparison.png** ‚Äî Word clouds –¥–ª—è toxic vs non-toxic
7. **bar_chart_top_words.png** ‚Äî –¢–æ–ø-20 —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º

---

## 5. –ß–µ–∫-–ª–∏—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è EDA

- [ ] –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç (47K —Å—Ç—Ä–æ–∫)
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (2 –∫–æ–ª–æ–Ω–∫–∏)
- [ ] –°–æ–∑–¥–∞–Ω –±–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫ `is_toxic`
- [ ] –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: ~8K not_toxic, ~39K toxic (5:1)
- [ ] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω: mean ~100-150 —Å–∏–º–≤–æ–ª–æ–≤
- [ ] –ü—Ä–æ–ø—É—Å–∫–∏: 0 (–∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã)
- [ ] –î—É–±–ª–∏–∫–∞—Ç—ã: < 5% (–∏–ª–∏ —É–¥–∞–ª–µ–Ω—ã)
- [ ] –¢–æ–ø —Å–ª–æ–≤ –ø–æ –∫–ª–∞—Å—Å–∞–º: –≤—ã—è–≤–ª–µ–Ω—ã –º–∞—Ä–∫–µ—Ä—ã
- [ ] Word clouds: –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã
- [ ] –ü—Ä–∏–º–µ—Ä—ã: 5 toxic + 5 non_toxic –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω—ã –≤—Ä—É—á–Ω—É—é
- [ ] –£—Ç–µ—á–∫–∏: –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã, –ª–æ–∂–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
- [ ] –Ø–∑—ã–∫: –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω English (–∏–ª–∏ –≤—ã—è–≤–ª–µ–Ω—ã –∏—Å–∫–ª—é—á–µ–Ω–∏—è)
- [ ] Data Contract: –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω
- [ ] –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: 7 –≥—Ä–∞—Ñ–∏–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
- [ ] –°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç: —Ä–∞—Å–ø–µ—á–∞—Ç–∞–Ω

---

## 6. –í—ã–≤–æ–¥—ã EDA (—à–∞–±–ª–æ–Ω)

```
EDA SUMMARY
===========

1. DATASET: 47,000 tweets, 2 columns (tweet_text, cyberbullying_type)

2. CLASS DISTRIBUTION:
   - Original: 6 balanced classes (~8K each)
   - Binary: 8K not_toxic (17%), 39K toxic (83%)
   - Imbalance ratio: 5:1 (requires class_weight='balanced')

3. TEXT LENGTH:
   - Mean: XXX characters, YY words
   - Min: X, Max: XXXX
   - Toxic comments: slightly longer/shorter

4. DATA QUALITY:
   - Missing values: 0 (clean)
   - Duplicates: X.X% (acceptable/removed)
   - Language: XX% English

5. KEY INSIGHTS:
   - Toxic words: [top 5 markers]
   - Non-toxic words: [top 5 markers]
   - Specific patterns: URLs X%, CAPS X%

6. DATA LEAKAGE:
   - Duplicates: None/Low (handled)
   - False correlations: [identified words]
   - Temporal: N/A (no timestamp)

7. RECOMMENDATIONS:
   - Use class_weight='balanced' for model training
   - Max features: 10,000 (TF-IDF)
   - Handle URLs/mentions in preprocessing
   - Consider removing duplicates before train/test split
```

---

## 7. Python Notebook Structure

```python
# EDA for Cyberbullying Classification Dataset
# =============================================

# 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from wordcloud import WordCloud
from langdetect import detect

# 2. Load Data
df = pd.read_csv('cyberbullying.csv')

# 3. Create Binary Target
df['is_toxic'] = df['cyberbullying_type'].apply(
    lambda x: 0 if x == 'not_cyberbullying' else 1
)

# 4. Basic Stats
# ... (–∫–æ–¥ –∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –≤—ã—à–µ)

# 5. Visualizations
# ... (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤ –ø–∞–ø–∫—É eda/)

# 6. Save EDA Report
# ... (—Å–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç)
```

---

**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** –í—ã–ø–æ–ª–Ω–∏—Ç—å EDA, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Å–∞–π—Ç—ã –≤ –æ—Ç—á—ë—Ç–µ.
